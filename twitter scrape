from selenium import webdriver
import pandas as pd
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
import time
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait


web = "https://twitter.com/CommunityNotes/status/1628158167006994436?cxt=HHwWiIDT6b_OsJgtAAAA"
path = "C:\\Users\\USER\\OneDrive\\Documents\\PERSONAL\\PERSONAL DEVELOPMENT\\DATA SCIENCE\\Web_Scraping_Tutorial\\chromedriver.exe"

options = webdriver.ChromeOptions()
options.add_experimental_option("detach", True)
service = Service(executable_path=path)
driver = webdriver.Chrome(service=service, options=options)
driver.get(web)
driver.maximize_window()
# time.sleep(8)

# tweets = driver.find_elements(By.XPATH, "//article[@role='article']")
#
# def get_tweets(element):
#     try:
#         user = element.find_element(By.XPATH, './/span[contains(text(),"@")]').text
#         text = element.find_element(By.XPATH, './/div[@lang]').text
#         stats = element.find_elements(By.XPATH, './/span[@data-testid]')
#         comments_data = stats[0].text
#         retweets_data = stats[1].text
#         likes_data = stats[2].text
#         views_data = stats[3].text
#     except:
#         tweet_data = ['user', 'text', 'comments', 'retweet', 'likes', 'views']
#
#     tweet_data = [user, text, comments_data, retweets_data, likes_data, views_data]
#
#     return tweet_data
#
#
# user_data = []
# texts = []
# comments_data = []
# retweets_data = []
# likes_data = []
# views_data = []
#
# tweets = WebDriverWait(driver, 8).until(EC.presence_of_all_elements_located((By.XPATH, "//article[@role='article']")))
#
# for tweet in tweets:
#     tweet_data = get_tweets(tweet)
#     # user = tweet.find_element(By.XPATH, './/span[contains(text(),"@")]').text
#     # text = tweet.find_element(By.XPATH, './/div[@lang]').text
#     # comments = tweet.find_elements(By.XPATH, './/span[@data-testid]')
#     # retweets = tweet.find_elements(By.XPATH, './/span[@data-testid][1]')
#     # likes = tweet.find_elements(By.XPATH, './/span[@data-testid][2]')
#     # views = tweet.find_elements(By.XPATH, './/span[@data-testid][3]')
#     user_data.append(tweet_data[0])
#     texts.append(("".join(tweet_data[1]).split()))
#     comments_data.append(tweet_data[2])
#     retweets_data.append(tweet_data[3])
#     likes_data.append(tweet_data[4])
#     views_data.append(tweet_data[5])
#
# driver.quit()
#
# tweet_df = pd.DataFrame({'user': user_data, 'text': texts, 'comments': comments_data, 'retweets': retweets_data, 'likes': likes_data, 'views': views_data})
# tweet_df.to_csv('tweetbot1.csv', index=False)
# print(tweet_df)

def get_tweets(element):
    try:
        user = element.find_element(By.XPATH, './/span[contains(text(),"@")]').text
        text = element.find_element(By.XPATH, './/div[@lang]').text
        stats = element.find_elements(By.XPATH, './/span[@data-testid]')
        comments_data = stats[0].text
        retweets_data = stats[1].text
        likes_data = stats[2].text
        views_data = stats[3].text
    except:
        tweet_data = ['user', 'text', 'comments', 'retweet', 'likes', 'views']

    tweet_data = [user, text, comments_data, retweets_data, likes_data, views_data]

    return tweet_data


user_data = []
texts = []
comments_data = []
retweets_data = []
likes_data = []
views_data = []


last_height = driver.execute_script("return document.body.scrollHeight")

while True:

    tweets = WebDriverWait(driver, 8).until(EC.presence_of_all_elements_located((By.XPATH, "//article[@role='article']")))

    for tweet in tweets:
        tweet_data = get_tweets(tweet)
        # user = tweet.find_element(By.XPATH, './/span[contains(text(),"@")]').text
        # text = tweet.find_element(By.XPATH, './/div[@lang]').text
        # comments = tweet.find_elements(By.XPATH, './/span[@data-testid]')
        # retweets = tweet.find_elements(By.XPATH, './/span[@data-testid][1]')
        # likes = tweet.find_elements(By.XPATH, './/span[@data-testid][2]')
        # views = tweet.find_elements(By.XPATH, './/span[@data-testid][3]')
        user_data.append(tweet_data[0])
        texts.append(("".join(tweet_data[1]).split()))
        comments_data.append(tweet_data[2])
        retweets_data.append(tweet_data[3])
        likes_data.append(tweet_data[4])
        views_data.append(tweet_data[5])

    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(8)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    else:
        last_height = new_height

driver.quit()

tweet_df = pd.DataFrame({'user': user_data, 'text': texts, 'comments': comments_data, 'retweets': retweets_data, 'likes': likes_data, 'views': views_data})
tweet_df.to_csv('tweetbot2.csv', index=False)
print(tweet_df)